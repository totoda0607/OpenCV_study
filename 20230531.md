# 투명망토 영상 출력하기

> * 비디오
> * 배경 이미지 캡처
> * 카메라 열고 읽기
> * Color segmentation 원하는 색상을 설정 : mask
> * 원하는 색상의 설정을 제거하기

```python
import cv2
import numpy as np
import time, argparse

parser = argparse.ArgumentParser()
parser.add_argument('--video', help='Input video path')
args = parser.parse_args()

cap = cv2.VideoCapture(args.video if args.video else 0)
time.sleep(3)

for i in range(60):
    ret, background = cap.read()

fourcc = cv2.VideoWriter_fourcc('m','p','4','v')
out = cv2.VideoWriter('video/output.mp4', fourcc, cap.get(cv2.CAP_PROP_FPS), (background.shape[1], background.shape[0]))
out2 = cv2.VideoWriter('video/original.mp4', fourcc, cap.get(cv2.CAP_PROP_FPS), (background.shape[1], background.shape[0]))

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)

    lower_red = np.array([0, 20, 30])
    upper_red = np.array([10, 100, 100])
    mask1 = cv2.inRange(hsv, lower_red, upper_red)

    lower_red = np.array([130, 54, 99])
    upper_red = np.array([180, 255, 255])
    mask2 = cv2.inRange(hsv, lower_red, upper_red)

    mask = mask1 + mask2

    mask_cloak = cv2.morphologyEx(mask1, op=cv2.MORPH_OPEN, kernel=np.ones((3, 3), np.uint8), iterations=2)
    mask_cloak = cv2.dilate(mask_cloak, kernel=np.ones((3, 3), np.uint8), iterations=1)
    mask_bg = cv2.bitwise_not(mask_cloak)

    cv2.imshow('mask_cloak', mask_cloak)

    res1 = cv2.bitwise_and(background, background, mask=mask_cloak)
    res2 = cv2.bitwise_and(frame, frame, mask=mask_bg)
    result = cv2.addWeighted(src1 = res1, alpha  = 1, src2 = res2, beta = 1, gamma = 0) 

    cv2.imshow('res1', res1)

    cv2.imshow('result', result)
    out.write(result)
    out2.write(frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
out.release()
out2.release()
cv2.destroyAllWindows()
```
> * mask의 값의 변환에 따라서 동작의 결과가 다르게 된다.

# 스도쿠 제작 코드

* 참고자료 : https://github.com/AntonSangho/mnist_sudoku_generator

# 하나의 이미지를 여러 이미지로 채우기

> * 1. 이미지를 가져오고 사이즈를 조정
> * 2. 조각에 맞는 이미지를 미리 확인
> * 3. 픽셀의 분포 확인
> * 4. 글씨 이미지와 고양이 픽셀 이미지 매칭
> * 5. 그림을 채워주기

```python
import cv2, os
import numpy as np
import matplotlib.pyplot as plt
from collections import OrderedDict, defaultdict

img_path = 'img/09.jpg'
img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
img = cv2.resize(img, dsize = None, fx= 0.2, fy = 0.2)
# print(img.shape)
# plt.figure(figsize=(20, 20))
# plt.axis('off')
# plt.imshow(img, cmap='gray')
# plt.show()

sample_imgs = np.load('dataset/k49-train-imgs.npz')['arr_0']
plt.figure(figsize=(20, 10))
for i in range(80):
    img_patch = 255 - sample_imgs[i]
    plt.subplot(5, 16, i+1)
    plt.title(int(np.mean(img_patch)))
    plt.axis('off')
    plt.imshow(img_patch, cmap='gray')
# plt.show()

mean = np.mean(255-sample_imgs, axis=(1,2))
# plt.figure(figsize=(12, 6))
# plt.hist(mean, bins=50, log=True)
# plt.show()

img = cv2.normalize(img, dst=None, alpha=120, beta=245, norm_type=cv2.NORM_MINMAX)
# plt.figure(figsize=(12, 6))
# plt.hist(img.flatten(), bins=50, log=True)
# plt.show()

bins = defaultdict(list)

for img_patch, mean in zip(sample_imgs, mean):
    bins[int(mean)].append(img_patch)

# print(len(bins))
h, w = img.shape

img_out = np.zeros((h*28, w*28), dtype=np.uint8)
for y in range(h):
    for x in range(w):
        level = img[y, x]
        b = bins[level]

        while len(b) == 0:
            level += 1
            b = bins[level]
        
        img_patch = 255 - b[np.random.randint(len(b))]
        img_out[y*28:(y+1)*28, x*28:(x+1)*28] = img_patch
plt.figure(figsize=(20, 20))
plt.axis('off')
plt.imshow(img_out, cmap='gray')
plt.show()

_ = cv2.imwrite('result/%s_bw.jpg' % os.path.splitext(os.path.basename(img_path))[0], img_out)
```
> * img_path, 과 sample_imgs의 데이터를 수정하여 다른 사진의 결과값을 확인 할 수 있다.
> * cv2.normalize를 통해서 흑, 백의 분포도를 원하는 범위로 간추려 줄 수 있다.
> * defaultdict 에서 patch이미지를 평균값에 맞춰서 bins에 딕셔너릴로 정리한다.
> * for문을 반복으로 돌면서 레벨에 해당되는 이미지를 채워 넣는다.
> * 이미지들 중에 랜덤으로 뽑아서 img_patch라는 변수에 넣는다.

# 이미지 매칭
* 서로 다른 두 이미지를 비교해서 짝이 맞는 형태의 객체가 있는 찾아내는 기술

* 평균 해시 매치 ( Average Hash Matching )
  * 이미지를 가로 세로 비율과 무관하게 특정한 크기로 축소합니다.
  * 픽셀 전체의 평균값을 구해서 각 픽셀의 값이 평균보다 작으면 0, 크면 1로 바꿉니다.
  * 0 또는 1로만 구성된 각 픽셀 값을 1행 1열로 변환합니다. (이는 한 개의 2진수 숫자로 볼 수 있습니다.)
* 평균 해시를 다른 이미지의 것과 비교해서 얼마나 비슷한 정도를 측정하는 방법으로는 유클리드 거리, 와 해밍 거리가 있다.
  * 유클리드 거리는 두 값의 차이로 거리를 계산합니다.
  * 해밍 거리는 두 값의 길이가 같아야 계산할 수 있다.
       * 해밍 거리는 두 수의 같은 자리 값 중 서로 다른 거이 몇개인지를 판단하여 유사도를 계산한다.

* 템플릿 매칭 ( Template Matching )
    * 템플릿 매칭은 특정 물체에 대한 이미지를 준비해 두고 그 물체가 포함 되어 있을 것이라고 예상할 수 있는 이미지와 비교하여 매칭되는 위치를 찾는 것이다.
    * 미리 준비한 이미지를 템플릿 이미지라고 한다.
    
    * result = cv2.matchTemplate(img, templ, method, result, mask)
        * img : 입력 이미지
        * templ : 템플릿 이미지
        * method : 매칭 메서드 (cv2.TM_SQDIFF : 제곱 차이 매칭, 완벽 매칭 : 0, 나쁜 매칭 : 큰값    
                             /cv2.TM_SQDIFF_NORMED : 제곱 차이 매칭의 정규화   
                             /cv2.TM_CCORR : 상관관계 매칭, 완벽매칭 : 큰값, 나쁜 매칭 : 0    
                             / cv2.TM_CCORR_NORMED : 상관관계 매칭의 정규화    
                             / cv2.TM_CCOEFF : 상관계수 매칭, 완벽 매칭 : 1, 나쁜 매칭 : -1    
                             / cv2.TM_CCOEFF_NORMED : 상관계수 매칭의 정규화)
        * result(optional) : 매칭 결과, (W - w + 1)x(H - h + 1)크기의 2차원 배열 [여기서 W,H는 입력 이미지의 너비와 높이, w, h는 템플릿 이미지의 너비와 높이]
        * mask(optional) : TM_SQDIFF, TM_CCORR_NORMED인 경우 사용할 마스크
        
    * minVal, maxVal, minLoc, maxLoc = cv2.minMaxLoc(src, mask)
        * src : 입력 1채널 배열
        * minVal, maxVal : 배열 전체에서의 최소 값, 최대값
        * minLoc, maxLoc : 최소 값과 최대 값의 좌표 (x, y)
        
    * cv2.matchTemplate() 함수는 입력 이미지(img)에서 템플릿 이미지(templ)를 슬라이딩하면서 주어진 메서드에 따라 매칭을 수행한다.
    * 배열의 최솟값 혹은 최댓값을 구하면 원하는 최선의 매칭값과 매칭점을 손쉽게 해주는 함수가 바로 cv2.minMaxLoc()이다. 이 함수는 입력 배열에서의 최솟값, 최댓값뿐만 아니라 최솟값, 최댓값의 좌표도 반환한다.

```python
import cv2
import numpy as np
import glob

img = cv2.imread('img/gun.jpg')
cv2.imshow('query', img)

search_dir = 'img/101_ObjectCategories'

def img2hash(img):
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    gray = cv2.resize(gray, (16, 16))
    avg = gray.mean()
    bi = 1 * (gray > avg)
    return bi

def hamming_distance(a, b):
    a = a.reshape(1, -1)
    b = b.reshape(1, -1)
    distance = (a != b).sum()
    return distance

query_hash = img2hash(img)

img_path = glob.glob(search_dir + '/**/*.jpg')
for path in img_path:
    img = cv2.imread(path)
    cv2.imshow('searching ...', img)
    cv2.waitKey(5)
    a_hash = img2hash(img)
    dst = hamming_distance(query_hash, a_hash)

    if dst/256 < 0.25:
        print(path, dst/256)
        cv2.imshow(path, img)
cv2.destroyWindows('searching')
cv2.waitKey(0)
cv2.destroyAllWindows()

```





















