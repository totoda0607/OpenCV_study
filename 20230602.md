# 우분투 디렉토리 용량 관리
> * du 명령어
> > * du -b --max-depth=1 . | sort -hr을 사용하면 특정 용량만 출력된다.
> > * du -b -h --max-depth=1 . | sort -hr 보기 편안한 방식으로 출력해준다.
> * top 명령어
> > * shift + p 

# 얼굴을 검출하기
* dlib : C++로 작성됨 범용 크로스 플랫폼 소프트웨어 라이브러리
* 얼굴을 우선적으로 잡고, 대략적인 위치를 정하고, 대략적인 위치의 좌표를 정해준다.

> * git clone -b test --single-branch https://github.com/AntonSangho/annoying-orange-face.git 로 파일받아오기
> * https://github.com/davisking/dlib-models/blob/master/shape_predictor_68_face_landmarks.dat.bz2 파일 받아오기
> * 사용하고자하는 annoying-orange-face에 bz2파일을 옮긴다.
> * bunzip2 shape_predictor_68_face_landmarks.dat.bz2 로 압축을 풀어준다.

* 과일 이미지 크기조절
* 데이터 셋을 가져오기
* 카메라를 열고, 인식을 해준다.
* 얼굴 인식을 해주고, 눈, 입을 인식한다.
* 인식 데이터를 crop으로 오려주고, 합성을 한다.(cv2.seamlessClone)

```python
import cv2
import numpy as np
import dlib
from imutils import face_utils, resize

orange_img = cv2.imread('apple.jpg')
orange_img = cv2.resize(orange_img, dsize = (512, 512))

detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')

# cam version
cap = cv2.VideoCapture(0)
# video version
# cap = cv2.VideoCapture('01.mp4')

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break
    # face selated from frame
    face = detector(frame)
    result = orange_img.copy()

    # face is over ones
    if len(face) > 0:
        face = face[0]
        # face of left, rigth, top, bottom is save
        x1, y1, x2, y2 = face.left(), face.top(), face.right(), face.bottom()
        # just face copyright
        face_img = frame[y1: y2, x1: x2].copy()

        shape = predictor(frame, face)
        shape = face_utils.shape_to_np(shape)

        for p in shape:
            cv2.circle(face_img, center = (p[0]-x1, p[1]-y1), radius=2, color=255, thickness=-1)

        # left eye
        le_x1 = shape[36, 0]
        le_y1 = shape[37, 1]
        le_x2 = shape[39, 0]
        le_y2 = shape[41, 1]

        le_margin = int((le_x2 - le_x1) * 0.18)

        # right eye
        re_x1 = shape[42, 0]
        re_y1 = shape[43, 1]
        re_x2 = shape[45, 0]
        re_y2 = shape[47, 1]

        re_margin = int((re_x2 - re_x1) * 0.18)

        # Crop
        left_eye_img = frame[le_y1-le_margin: le_y2+le_margin, le_x1-le_margin: le_x2+le_margin].copy()
        right_eye_img = frame[re_y1-re_margin: re_y2+re_margin, re_x1-re_margin: re_x2+re_margin].copy()

        left_eye_img = resize(left_eye_img, width=100)
        right_eye_img = resize(right_eye_img, width=100)

        # poison blending
        result = cv2.seamlessClone(left_eye_img, result, np.full(left_eye_img.shape[:2], 255, left_eye_img.dtype), (200, 200), cv2.MIXED_CLONE)
        result = cv2.seamlessClone(right_eye_img, result, np.full(right_eye_img.shape[:2], 255, right_eye_img.dtype), (350, 200), cv2.MIXED_CLONE)


        # mouse
        mouse_x1 = shape[48, 0]
        mouse_y1 = shape[50, 1]
        mouse_x2 = shape[54, 0]
        mouse_y2 = shape[57, 1]

        mouse_margin = int((mouse_x2 - mouse_x1) * 0.1)
        mouse_img = frame[mouse_y1-mouse_margin: mouse_y2+mouse_margin, mouse_x1-mouse_margin: mouse_x2+mouse_margin].copy()
        mouse_img = resize(mouse_img, width=250)
        result = cv2.seamlessClone(mouse_img, result, np.full(mouse_img.shape[:2], 255, mouse_img.dtype), (280, 320), cv2.MIXED_CLONE)

        # cv2.imshow('left eye', left_eye_img)
        # cv2.imshow('right eye', right_eye_img)
        # cv2.imshow('mouse', mouse_img)
        cv2.imshow('result', result)
    
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break
```

> * imutils 가 없는경우 pip3 install imutils로 설치를 해준다.

# Snow어플과 비슷한 효과

```python
import cv2, dlib, sys
import numpy as np

# video scaler size down
scaler = 0.3

# initialize face detector and shape predictor
detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')

# 1. load video
# cap = cv2.VideoCapture('samples/girl.mp4')
cap = cv2.VideoCapture(0)
# load overlay image
overlay = cv2.imread('samples/ryan_transparent.png', cv2.IMREAD_UNCHANGED)

# overlay function
def overlay_transparent(background_img, img_to_overlay_t, x, y, overlay_size=None):
  try:
    bg_img = background_img.copy()
    # convert 3 channels to 4 channels
    if bg_img.shape[2] == 3:
      bg_img = cv2.cvtColor(bg_img, cv2.COLOR_BGR2BGRA)

    if overlay_size is not None:
      img_to_overlay_t = cv2.resize(img_to_overlay_t.copy(), overlay_size)

    b, g, r, a = cv2.split(img_to_overlay_t)

    mask = cv2.medianBlur(a, 5)

    h, w, _ = img_to_overlay_t.shape
    roi = bg_img[int(y-h/2):int(y+h/2), int(x-w/2):int(x+w/2)]

    img1_bg = cv2.bitwise_and(roi.copy(), roi.copy(), mask=cv2.bitwise_not(mask))
    img2_fg = cv2.bitwise_and(img_to_overlay_t, img_to_overlay_t, mask=mask)

    bg_img[int(y-h/2):int(y+h/2), int(x-w/2):int(x+w/2)] = cv2.add(img1_bg, img2_fg)

    # convert 4 channels to 4 channels
    bg_img = cv2.cvtColor(bg_img, cv2.COLOR_BGRA2BGR)

    return bg_img
  except Exception:return background_img

face_roi = []
face_sizes = []

# loop
# 2. 비디오를 계속 프레임 단위로 읽어야하므로
while True:
  # read frame buffer from video
  ret, img = cap.read()
  # 프레임이 없으면 종료 
  if not ret:
    break

  # resize frame
  img = cv2.resize(img, (int(img.shape[1] * scaler), int(img.shape[0] * scaler)))
  ori = img.copy()

  # find faces
  if len(face_roi) == 0:
    faces = detector(img, 1)
  else:
    roi_img = img[face_roi[0]:face_roi[1], face_roi[2]:face_roi[3]]
    # cv2.imshow('roi', roi_img)
    faces = detector(roi_img)

  # no faces
  if len(faces) == 0:
    print('no faces!')

  # find facial landmarks
  for face in faces:
    if len(face_roi) == 0:
      dlib_shape = predictor(img, face)
      shape_2d = np.array([[p.x, p.y] for p in dlib_shape.parts()])
    else:
      dlib_shape = predictor(roi_img, face)
      shape_2d = np.array([[p.x + face_roi[2], p.y + face_roi[0]] for p in dlib_shape.parts()])

    # face effect
    for s in shape_2d:
      cv2.circle(img, center=tuple(s), radius=1, color=(255, 255, 255), thickness=2, lineType=cv2.LINE_AA)

    # compute face center
    #center_x, center_y = np.mean(shape_2d, axis=0).astype(np.int)
    center_x, center_y = np.mean(shape_2d, axis=0).astype(np.int32)

    # compute face boundaries
    min_coords = np.min(shape_2d, axis=0)
    max_coords = np.max(shape_2d, axis=0)

    # draw min, max coords
    cv2.circle(img, center=tuple(min_coords), radius=1, color=(255, 0, 0), thickness=2, lineType=cv2.LINE_AA)
    cv2.circle(img, center=tuple(max_coords), radius=1, color=(255, 0, 0), thickness=2, lineType=cv2.LINE_AA)

    # compute face size
    face_size = max(max_coords - min_coords)
    face_sizes.append(face_size)
    if len(face_sizes) > 10:
      del face_sizes[0]
    mean_face_size = int(np.mean(face_sizes) * 1.8)

    # compute face roi
    face_roi = np.array([int(min_coords[1] - face_size / 2), int(max_coords[1] + face_size / 2), int(min_coords[0] - face_size / 2), int(max_coords[0] + face_size / 2)])
    face_roi = np.clip(face_roi, 0, 10000)

    # draw overlay on face
    result = overlay_transparent(ori, overlay, center_x + 8, center_y - 25, overlay_size=(mean_face_size, mean_face_size))

  # visualize
  cv2.imshow('original', ori)
  cv2.imshow('facial landmarks', img)
  cv2.imshow('result', result)
  if cv2.waitKey(1) == ord('q'):
    sys.exit(1)
```

# 동물들의 얼굴을 포커싱 하기

```python
import dlib, cv2, os
from imutils import face_utils
import numpy as np
import matplotlib.pyplot as plt

# 모델 넣기 (https://github.com/tureckova/Doggie-smile)
# 강아지가 정면으로 볼때 사진을 찍음 
detector = dlib.cnn_face_detection_model_v1('dogHeadDetector.dat')
predictor = dlib.shape_predictor('landmarkDetector.dat')

# 강아지 사진 불러오기 
img_path = 'img/18.jpg'
filename, ext = os.path.splitext(os.path.basename(img_path))
img = cv2.imread(img_path)
# opencv는 이미지를 로드하면 BGR형태이기 때문에 RGB형태로 변경해준다.
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
# img = cv2.resize(img, dsize=None, fx=0.5, fy=0.5)

# plt.figure(figsize=(16, 16))
# plt.imshow(img)
# plt.show()

# 얼굴 인식하기 
# detector에 img를 넣어준다
dets = detector(img, upsample_num_times=1)

print(dets)

img_result = img.copy()
# d.confidence는 몇퍼센트의 확율로 강아지이다 라는 자신감을 나타냄. 1.04이면 104%이다 
for i, d in enumerate(dets):
    print("Detection {}: Left: {} Top: {} Right: {} Bottom: {} Confidence: {}".format(i, d.rect.left(), d.rect.top(), d.rect.right(), d.rect.bottom(), d.confidence))
    #x1,y1은 시작점, x2,y2는 끝점 
    x1, y1 = d.rect.left(), d.rect.top()
    x2, y2 = d.rect.right(), d.rect.bottom()

    cv2.rectangle(img_result, pt1=(x1, y1), pt2=(x2, y2), thickness=2, color=(255,0,0), lineType=cv2.LINE_AA)
# 어둠고 줄에 가려져서 인식 못함    
# plt.figure(figsize=(16, 16))
# plt.imshow(img_result)
# plt.show()

# 눈, 코, 입을 찾기 (랜드마크) 
shapes = []

for i, d in enumerate(dets):
    # predictor안에 이미지와 아까 구한 사각형을 넣어준다.
    shape = predictor(img, d.rect)
    # shape에 numpy array를 넣어준다
    shape = face_utils.shape_to_np(shape)
    
    for i, p in enumerate(shape):
        shapes.append(shape)
        # 점찍힌 번호를 가지고 순서를 통해서 눈과 입을 구분할 수 있다
        cv2.circle(img_result, center=tuple(p), radius=3, color=(0,0,255), thickness=-1, lineType=cv2.LINE_AA)
        cv2.putText(img_result, str(i), tuple(p), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1, cv2.LINE_AA)

img_out = cv2.cvtColor(img_result, cv2.COLOR_RGB2BGR)
# cv2.imwrite('img/%s_out%s' % (filename, ext), img_out)
# plt.figure(figsize=(16, 16))
# plt.imshow(img_result)
# plt.show()

# 번호에 맞춰서 루돌프 뿔과 코를 넣기
from math import atan2, degrees

# overlay function
def overlay_transparent(background_img, img_to_overlay_t, x, y, overlay_size=None):
    img_to_overlay_t = cv2.cvtColor(img_to_overlay_t, cv2.COLOR_BGRA2RGBA)
    bg_img = background_img.copy()
    # convert 3 channels to 4 channels
    if bg_img.shape[2] == 3:
        bg_img = cv2.cvtColor(bg_img, cv2.COLOR_RGB2RGBA)

    if overlay_size is not None:
        img_to_overlay_t = cv2.resize(img_to_overlay_t.copy(), overlay_size)

    b, g, r, a = cv2.split(img_to_overlay_t)

    mask = cv2.medianBlur(a, 5)

    h, w, _ = img_to_overlay_t.shape
    roi = bg_img[int(y-h/2):int(y+h/2), int(x-w/2):int(x+w/2)]

    img1_bg = cv2.bitwise_and(roi.copy(), roi.copy(), mask=cv2.bitwise_not(mask))
    img2_fg = cv2.bitwise_and(img_to_overlay_t, img_to_overlay_t, mask=mask)

    bg_img[int(y-h/2):int(y+h/2), int(x-w/2):int(x+w/2)] = cv2.add(img1_bg, img2_fg)

    # convert 4 channels to 4 channels
    bg_img = cv2.cvtColor(bg_img, cv2.COLOR_RGBA2RGB)

    return bg_img

def angle_between(p1, p2):
    xDiff = p2[0] - p1[0]
    yDiff = p2[1] - p1[1]
    return degrees(atan2(yDiff, xDiff))

img_result2 = img.copy()

horns = cv2.imread('img/horns2.png',  cv2.IMREAD_UNCHANGED)
horns_h, horns_w = horns.shape[:2]

nose = cv2.imread('img/nose.png',  cv2.IMREAD_UNCHANGED)

for shape in shapes:
    horns_center = np.mean([shape[4], shape[1]], axis=0) // [1, 1.3]
    horns_size = np.linalg.norm(shape[4] - shape[1]) * 3
    
    nose_center = shape[3]
    nose_size = horns_size // 4

    angle = -angle_between(shape[4], shape[1])
    M = cv2.getRotationMatrix2D((horns_w, horns_h), angle, 1)
    rotated_horns = cv2.warpAffine(horns, M, (horns_w, horns_h))

    img_result2 = overlay_transparent(img_result2, nose, nose_center[0], nose_center[1], overlay_size=(int(nose_size), int(nose_size)))
    try:
        img_result2 = overlay_transparent(img_result2, rotated_horns, horns_center[0], horns_center[1], overlay_size=(int(horns_size), int(horns_h * horns_size / horns_w)))
    except:
        print('failed overlay image')

img_out2 = cv2.cvtColor(img_result2, cv2.COLOR_RGB2BGR)
cv2.imwrite('img/%s_out2%s' % (filename, ext), img_out2)
plt.figure(figsize=(16, 16))
plt.imshow(img_result2)
plt.show()
```
